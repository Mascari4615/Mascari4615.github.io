---
title: "🌗 Neural Networks & Backpropagation Algorithm - 신경망 & 역전파 알고리듬"
date: 2023-10-25. 10:07
last_modified_at: 2023-10-25. 10:07
categories: ⭐Computer 🌗AI
tags: AI Neural-Networks Backpropagation-Algorithm
---

10, 11차시  

### 💫 Neural Networks & Backpropagation Algorithm - 신경망 & 역전파 알고리듬

---

신경망과 역전파 알고리듬  
뉴런과 시냅스  

배우는 것 (구조, 학습 알고리듬)  
학습 알고리듬 : Error Backpropagation Learning Algorithm 역전파 학습 알고리듬을 통한  
Architectures 구조 : Feed-Forward 전진 전파 / Multi-Layer Neural Network 다층 신경망(뉴런적인) 네트워크  

역전파 알고리듬이 신경망 연구에 부활을 일으켰다?  
Resurgence 부활  

Topologies 이상 = Architectures 구조  

AI 매개변수가 ~개다 = 시냅스가 ~개다  

@ ⭐ 뉴런 - 입력은 여러 개, 출력은 한 개 (갈라지긴 하지만 값이 하나라는 뜻)  

시냅스 Like 수도꼭지  
신호량 통제/조절  

인공 뉴런  
가중치 W Weight == 시냅스  

1. 입력들을 각가의 가중치를 곱해 X를 구하고 더함 Weighted Sum
2. Activation Function 활성 함수를 통해 최종 값 계산 (X가 겁나 크니까 작게 찌그러뜨리기/조절하기)

뉴럴 네트워크, 다층 신경망  
중간층(구 은닉층)  

입력층의 뉴런은 모양이 다르다!?  
모든 뉴런은 뭔가 입력을 받아서 계산을 하고 뿌리는데,  
입력층 뉴런은 그냥 입력을 받아서 뿌리기만 함  
반대로 보면 입력을 받을 때 다 가중치를 곱한 값을 가져오는데, 입력층은 그런거 없이 그냥 가져옴  
그래서 엄밀히 말하면 입력층은 뉴런이 아님  

Feed-Forward  
입력을 계산해서 출력을 앞으로 앞으로 보낸다  

레이어 256개? 몇 백개?  
뇌도 레이어가 나뉜다 (뉴런이 집중되어 있거나 좀 옅거나)  

RNN, 순환 신경망은  
한 층에서 다음 층으로, 뉴런이 출력을 할 때  
자기 자신에게도 출력값을 입력함 (Feedback)  

i번째 뉴런과 j번째 뉴런을 잇는 가중치 Wij  

무튼 사용하는 입장에서는 뉴럴 네트워크 안의 과정/원리를 모른채  
I.E. 대화 인공지능에 질문을 하면, 뭔가 처리되고, 답변이 옴  

우리가 원하는 값을 찾기 위해서 직접 가중치를 조정해야 하는 거면 안쓰지  
우리는 입력 출력 페어만 전달하고, 그 가중치를 찾는 과정을 AI가  

오류 역전파 학습 알고리듬  
오류? 우리가 AI에게 준 입력 출력 페어랑 출력이 다른 거  
그걸 이용해서 다시 가중치를 수정하는 알고리듬  

중간-출력층 사이의 가중치 조정  
Wij = Wjk + △Wjk  
△델타Wjk를 아는 값과 (이상 - 에러 오차)를 이용하여 계산  

입력-중간층 사이의 가중치 조정  
Wij = Wjk + △Wjk  
△델타Wjk를 아는 값과 (중간-출력층 사이의 가중치 조정 과정에서 구한 값)을 이용하여 계산  

역전파  
-> 입력-중간층 사이 가중치를 고치기 위해서, 중간-출력층 사이 가중치를 먼저 고치고 나온 값을 이용  

여기서 문제  
여러 층을 거쳐 옮겨오다보면 제대로 동작 안함?  

그래서 그 문제를 해결하기 위해 나온게 딥러닝  

알고리듬을 보면 그냥 더하기만 있음 (곱하기도 더하기)  
간단한 계산, 그래서 한 번에 많은 단순 계산을 할 수 있는 GPU를 사용  

답을 아는 경우  
답을 모르면 강화학습, 비지도 학습?  

@ 이러한 신경망 구조가 어떻게 되냐?  

@ 다음 층의 뉴런과 모두 연결된다
@ 맨 위, 맨 아래 뉴런 빼고 모두 생략해서 그리기  

Feed-Foward -> Back Backpropagation  
